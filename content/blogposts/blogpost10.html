---
layout: page
title: Blog Post
pageHeading: Code Red - A large scale study on ChatGPT generated code vulnerabilities
permalink: /blogposts/blog10.html
---

<main id="main">
  <section>
    <div class="container">
      <div class="section-title">
        <h2>{{ page.pageHeading }} </h2>
        <code>Author: UiO</code> <br />
      </div>
      <div class="col-sm-12">
        <p>
          Recent observations have shown that Large Language Models <b>(LLMs), when employed for code generation, can inadvertently produce vulnerable code.</b> This issue has been highlighted in various small-scale studies, including blog posts and research articles, demonstrating its potential risks. However, the broader implications for software engineering remained unclear, while many developers now rely on AI tools for code generation and auto-completion. Currently, ChatGPT is at the forefront as a leading proprietary technology in LLMs. Understanding the extent to which it is affected by generating vulnerable code, is a crucial question that needs addressing.
        </p>
        <p>
          A recent large-scale study authored by <b><a href="https://www.linkedin.com/in/norberttihanyi">Norbert Tihanyi, PhD</a>, <a href="https://www.linkedin.com/in/tb-link/">Tamas Bisztray, PhD</a> , <a href="https://www.linkedin.com/in/ACoAABw0yVkBXLZseH1ZhmcgkwSkzRUa8R2njGg">Ridhi Jain</a>, <a href="https://www.linkedin.com/in/ACoAAAk163sB8JBZb2mni8KW5JADXlIevkV0l1c">Mohamed Amine Ferrag, PhD</a>, <a href="https://www.linkedin.com/in/ACoAAACgMAEB5TjQX-YAfZL8M97EHQz9k1uYtYg">Lucas Cordeiro</a></b> and <b><a href="https://www.linkedin.com/in/vasileiosmavroeidis">Vasileios Mavroeidis</a></b> unveiled some concerning findings, as <b>C programs generated by GPT-3.5 exhibit vulnerabilities 51.24% of the time across various programming scenarios</b>, posing a significant threat to software safety and security. Their research paper, titled “<b>The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification</b>” [1], goes beyond these alarming statistics. <b>It introduces a method for assessing the propensity of various Large Language Models (LLMs) to generate vulnerable code.</b>
        </p>
        <p>
          This blog post will delve into the crucial insights of this paper.
        </p>
        <p>
          The research questions raised were the following: 
        </p>
        <ul>
          <li>
            RQ1: <b>How likely is purely LLM-generated code to contain vulnerabilities</b> on the first output when using simple zero- shot text-based prompts? 
          </li>
          <li>
            RQ2: <b>What are the most typical vulnerabilities</b> LLMs introduce when generating code?
          </li>
        </ul>
        <p>
          To accurately address these questions, it was essential to examine a substantial corpus of AI-generated code. Before this study, all extensive C program code databases were either sourced from real-world projects authored by human developers or synthetically designed to include specific vulnerabilities for machine learning or related objectives. <b>The research paper also introduced the FormAI dataset, marking the first large-scale AI-generated database focusing on vulnerable C programs.</b> Merely generating C programs without classifying vulnerabilities would not suffice to validate the research hypotheses. The methodology adopted in the paper was as follows:
        </p>
        <ol>
          <li>Generate a diverse set of C program codes for a wide range of programming tasks.</li>
          <li>Reduce redundancy and code replication with a novel zero shot prompting template.</li>
          <li>Perform vulnerability classification without introducing false positives.</li>
        </ol>
        <p>
          To facilitate the study, a <b>specialised prompting technique</b> was devised to interact with ChatGPT through API calls for various programming tasks as shown <b>in Figure 1</b>. These tasks, referred to as [Type], encompass a wide range of applications such as Wi-Fi strength checkers, terminal-based games, scientific calculators, cryptography protocols, and more, randomly chosen from a pool of 200 coding tasks. To enhance diversity, each prompt was also paired with a selected randomly from a collection of 100 different coding styles.
        </p>
        <div style="width: 80%; margin-left:auto; margin-right: auto;">
          <img
          src="{{ site.baseurl }}/assets/img/blog10-figure1.png"
          alt="Dynamic code generation prompt."
          class="responsive-img-80vh" 
          />
        </div>
        <p><b>Figure 1:</b> Dynamic code generation prompt.</p>
        <p>
          The process of the dataset creation can be seen in Figure 2. <b>In this case the LLM module is GPT-3.5-turbo</b>. After the dataset has been populated with compliable C programs, the important task of vulnerability classification and detection begins. Due to the impracticality of manually labelling the entire dataset, given its extensive size, the authors opted to utilize the <b>Efficient SMT-based Bounded Model Checker (ESBMC) to perform the vulnerability labelling</b>. This tool can formally verify the presence of specific vulnerabilities in the data through symbolic execution. A constraint of this approach is its dependency on the available computational resources, which limits the search depth to a predefined boundary. This restriction means it can only identify vulnerabilities within this set range, owing to the method's intensive resource demands.
        </p>
        <div style="width: 80%; margin-left:auto; margin-right: auto;">
          <img
          src="{{ site.baseurl }}/assets/img/blog10-figure2.png"
          alt="AI-driven Dataset Generation and Vulnerability Labeling with Program Classification by the BMC Module."
          class="responsive-img-80vh" 
          />
        </div>
        <p><b>Figure 2:</b> AI-driven Dataset Generation and Vulnerability Labeling with Program Classification by the BMC Module.</p>
        <p>
          In the case of FormAI, individual C programs in the dataset might contain even more vulnerabilities then detected. The dataset encompasses over 8.8 million lines of C code, typically 79 lines per sample, with 47-line programs being most common. <b>An interesting property of the generated dataset was that the use of most common and least frequent C-keywords by ChatGPT, was similar to both human written and synthetic datasets as shown in Figure 3</b>.
        </p>
        <div style="width: 80%; margin-left:auto; margin-right: auto;">
          <img
          src="{{ site.baseurl }}/assets/img/blog10-figure3.png"
          alt="C Keyword frequency in FormAI, SARD, and BigVul."
          class="responsive-img-80vh" 
          />
        </div>
        <p><b>Figure 3:</b> C Keyword frequency in FormAI, SARD, and BigVul.</p>
        <p>
          Counting the 32 C keywords with a token-based frequency counter, normalizing their frequency per million lines shows, that the distribution of if-statements, loops, and variables is similar to real-world projects. This is likely due to that GPT models are being trained on actual GitHub projects and human created code. Define Σ as the set of all C samples, Σ = {c1,c2,...,c112,000}. The vulnerabilities identified by ESBMC module in the FormAI dataset were classified into 4 main categories:
        </p>
        <ul>
          <li>
            VS ⊆ Σ: the set of samples for which verification was successful (no vulnerabilities have been detected within the bound k);
          </li>
          <li>
            VF ⊆ Σ: the set of samples for which the verification status failed (known counterexamples);
          </li>
          <li>
            TO ⊆ Σ: the set of samples for which the verification process was not completed within the provided time frame (as a result, the status of these files remains uncertain);
          </li>
          <li>
            ER ⊆ Σ: the set of samples for which the verification status resulted in an error.
          </li>
        </ul>
        <p>
          <b>The category VF is the most interesting for our purposes, as it signifies a case where one or more vulnerabilities were found.</b> In such cases ESBMC found a “counterexample” during the symbolic execution. The category VF was divided into 9 subcategories:
        </p>
        <ul>
          <li>BOF : Buffer overflow on scanf()/fscanf()</li>
          <li>DFN : Dereference failure : NULL pointer</li>
          <li>DFA : Dereference failure : array bounds violated</li>
          <li>ARO : Arithmetic overflow</li>
          <li>ABV : Array bounds violated</li>
          <li>DFI : Dereference failure : invalid pointer</li>
          <li>DFF : Dereference failure : forgotten memory</li>
          <li>OTV : Other vulnerabilities</li>
          <li>DBZ : Division by zero
          </li>
        </ul>
        <p>
          Through linking the vulnerabilities to Common Weakness Enumeration (CWE) identifiers, <b>41 unique CWEs were identified. Among these, eight CWE identifiers are featured in MITRE's Top 25 CWEs for 2022. In sequence, these include: CWE-787 (1st), CWE-20 (4th), CWE-125 (5th), CWE-416 (7th), CWE-476 (11th), CWE-190 (13th), CWE-119 (19th), and CWE-362 (22nd)</b>. Within FormAI, it's common for a single file to encompass several vulnerabilities. Moreover, a single vulnerability might be associated with several CWEs. Table 1 offers an overview, listing the identified CWEs along with the frequency of their occurrences within the dataset.
        </p>
        <div style="width: 80%; margin-left:auto; margin-right: auto;">
          <img
          src="{{ site.baseurl }}/assets/img/blog10-table1.png"
          alt="The vulnerabilities identified by ESBMC, linked to Common Weakness Enumeration identifiers."
          class="responsive-img-80vh" 
          />
        </div>
        <p><b>Table 1:</b> The vulnerabilities identified by ESBMC, linked to Common Weakness Enumeration identifiers.</p>
        <b>
          The dataset comprises three distinct files:
        </b>
        <ul>
          <li>FormAI dataset C samples-V1.zip - This file contains all the 112,000 C files.</li>
          <li>FormAI dataset classification-V1.zip - This file contains a CSV file with the original code and vulnerability classification.</li>
          <li>FormAI dataset human readable-V1.csv - Human readable version without the code</li>
        </ul>
        <p>
          The dataset can be accessed on both GitHub and IEEE Dataport.
        </p>
        <ul>
          <li>GitHub: <a href="https://github.com/FormAI-Dataset/">https://github.com/FormAI-Dataset/</a> </li>
          <li>IEEE dataport: <a href="https://dx.doi.org/10.21227/vp9n-wv96">https://dx.doi.org/10.21227/vp9n-wv96</a>
          </li>
        </ul>
        <p>
          The paper drew the following conclusions:
        </p>
        <ul>
          <li>
            RQ1: How likely is purely LLM-generated code to contain vulnerabilities on the first output when using simple zero-shot text-based prompts? <b>Answer: At least 51.24% of the samples from the 112,000 C programs contain vulnerabilities. This indicates that GPT-3.5 often produces vulnerable code. Therefore, one should exercise caution when considering its output for real-world projects.</b>
          </li>
          <li>
            RQ2: What are the most typical vulnerabilities LLMs introduce when generating code? <b>Answer: For GPT-3.5: Arithmetic Overflow, Array Bounds Violation, Buffer Overflow, and various Dereference Failure issues were among the most common vulnerabilities. These vulnerabilities are pertinent to MITRE's Top 25 list of CWEs.</b>
          </li>
        </ul>
        <b>ACKNOWLEDGEMENTS</b>
        <p>
          The University of Oslo has received funding from the EU Connecting Europe Facility (CEF) programme under Grant Agreement No.INEA/CEF/ICT/A2020/2373266 (JCOP project) and the Horizon Europe programme under Grant Agreement No. 101070586 (PHOENi2X project). The ESBMC development is currently funded by ARM, Intel, EPSRC grants EP/T026995/1, EP/V000497/1, EU H2020 ELEGANT 957286, and Soteria project awarded by the UK Research and Innovation for the Digital Security by Design (DSbD) Programme.
        </p>
        <b>Resources</b>
        <p>[1] Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C. Cordeiro, and Vasileios Mavroeidis. 2023. The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification. In Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2023). Association for Computing Machinery, New York, NY, USA, 33-43. <a href="https://doi.org/10.1145/3617555.3617874">https://doi.org/10.1145/3617555.3617874</a></p>
      </div>
    </div>
  </section>
</main>
